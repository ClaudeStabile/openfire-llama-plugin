<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Welcome file</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li>
<ul>
<li><a href="#llama">LLaMA</a></li>
<li><a href="#overview">Overview</a></li>
<li><a href="#known-issues">Known Issues</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#how-to-use">How to use</a></li>
<li><a href="#further-information">Further information</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h2 id="llama">LLaMA</h2>
<p>AI Inference engine for Openfire</p>
<h2 id="overview">Overview</h2>
<img src="https://igniterealtime.github.io/openfire-llama-plugin/llama-summary.png">
<h2 id="known-issues">Known Issues</h2>
<p>This version has embedded binaries for only Linux 64 and Windows 64.</p>
<h2 id="installation">Installation</h2>
<p>copy llama.jar to the plugins folder</p>
<h2 id="configuration">Configuration</h2>
<img src="https://user-images.githubusercontent.com/110731/259781570-5a9a2918-ca51-4bed-80f0-b50db7aa63cb.png">
<h3 id="enable-llama">Enable LLaMA</h3>
<p>Enables or disables the plugin. Reload plugin or restart Openfire if this or any of the settings other settings are changed.</p>
<h3 id="enable-muc-groupchat-support">Enable MUC (Groupchat) Support</h3>
<p>If this option is not enabled, LLaMA will only use a single parent group called <em>public</em>. Subgroups will be enabled and all participants will have <em>operator</em> privileges. They can publish media streams and manage the meeting. See LLaMA documentation for full details. Both anonymous and  authenticated XMPP sessions can join a public group and all subgroups.</p>
<p>To join a public subgroup called my-meeting, with the llama web client, use <a href="http://your-openfire-server:7070/llama/?room=public/my-meeting&amp;username=your-user-name">http://your-openfire-server:7070/llama/?room=public/my-meeting&amp;username=your-user-name</a>.</p>
<p>If this option is enabled, then LLaMA LLaMA becomes integrated with Openfire. Openfire MUC group-chat rooms can be used as LLaMA groups in addition to the default public group. Authentication is enabled and only authenticated XMPP sessions can join a LLaMA group subject to the room configuration. For example, member only MUC rooms will disable allow-anonymous in LLaMA and only room member xmpp sessions will be allowed to join the LLaMA group.</p>
<p>In this mode, users will require an authenticated XMPP session to join an Openfire MUC groupchat room. See Pade client or the LLaMA plugin for ConverseJS for more details.</p>
<p>Once a user has joined with at least presenter permissions, they can invite an external user with an invitation token.</p>
<p>To join an MUC groupchat room called lobby with the LLaMA web client as user <em>fred</em> using invitation token xxxxxxxxxxx, use <a href="http://localhost:7070/llama?token=xxxxxxxxxxx&amp;room=public&amp;username=fred">http://localhost:7070/llama?token=xxxxxxxxxxx&amp;room=public&amp;username=fred</a></p>
<h3 id="usernamepassword">Username/Password</h3>
<p>This is Openfire username/password for the user that will have LLaMA admin permissions to view stream stats and join any meeting. By default the user will be “llama-user” and the password witll be a random string. If you are using ldap or your Openfire user manager is in read-only mode and a new user cannot be created, then you must create the user and specify the username and password here…</p>
<h3 id="ip-addresstcp-port">IP Address/TCP Port</h3>
<p>This is the internal IP address of the network card to which you want LLaMA to bind to. LLaMA is not exposed outside of the internal network and can only be accessed via Openfire using an XMPP client connection. By default port 6060 will be used. However any other internal port can be used. Use <a href="http://ip-address">http://ip-address</a>:port on the internal network to confirm llama is up and running.</p>
<h3 id="udp-port-minmax">UDP Port Min/Max</h3>
<p>This limits the pool of ephemeral ports that ICE UDP connections can allocate from. This affects both host candidates, and the local address of server reflexive candidates.</p>
<h3 id="turn-ip-addressport">TURN IP Address/Port</h3>
<p>This is the public IP address of the FQDN of your openfire server that will be exposed to client web browsers when they ask for ICE canndidates during media negotiation. Make sure the port specified is opened for TCP and UDP. llama will bind its interrnal TURN server to this port. If LLaMA is behind NAT and your NAT device doesn’t support hairpinning, then you must use an external TURN server.</p>
<h3 id="external-url">External URL</h3>
<p>This is the base public URL (minus path) that is used to access llama externally by clients. The host name should resolve to the IP address specified for TURN above otherwise, ensure you have a proxy or http forwarding rule that ensures this happens.</p>
<h2 id="how-to-use">How to use</h2>
    </div>
  </div>
</body>

</html>
